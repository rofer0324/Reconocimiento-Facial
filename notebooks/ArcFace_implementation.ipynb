{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06c4ba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32a7c144",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcFace(nn.Module):\n",
    "    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False):\n",
    "        \"\"\"\n",
    "        \n",
    "        Implementacion de ArcFace\n",
    "        \n",
    "        Args:\n",
    "            in_features: Tamaño del embedding vector (input)\n",
    "            out_features: Números de clases\n",
    "            s: Factor de escalado\n",
    "            m: margen añadido entre clases en el espacio angular\n",
    "            easy_margin: Usar solamente si la version base se vuelve inestable\n",
    "        \"\"\"\n",
    "\n",
    "        super(ArcFace, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight) # inicializa la clase para los weights\n",
    "\n",
    "        # Calcula cos(m) y sen(m) para temas de eficencia\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = math.cos(math.pi - m) # umbral para el margen de decision\n",
    "        self.mm = math.sin(math.pi - m) * m # margen de penalización\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        \"\"\"\n",
    "        \n",
    "        Forward prop para ArcFace\n",
    "        \n",
    "        Args:\n",
    "            input: Inputs las dimensiones del embedding tensor [batch_size, in_features]\n",
    "            label: Labels con la dimension del [batch_size]\n",
    "        Returns:\n",
    "            Output: Logits (resultado tras pasar por una funcion de activacion) with shape [batch_size, out_features] to pass it to the CrossEntropyLoss\n",
    "        \"\"\" \n",
    "\n",
    "        # Normalize both inputs features and weight matrix\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight)) # Cosine similarity between features and weights\n",
    "        sine = torch.sqrt(1.0 - torch.clamp(cosine**2, 0, 1)) # sin(θ) from cos(θ)\n",
    "\n",
    "        # Compute cos(θ + m) using trigonometric identity\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "\n",
    "        # Decide whether to apply margin based on thresholding (remember just use it, if the model becomes unstable)\n",
    "        if self.easy_margin:\n",
    "            # Use cosine if it is positive, else keep original\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            # Use original phi only if above threshold, else apply modified margin\n",
    "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        \n",
    "        # One Hot to enconde labels\n",
    "        one_hot = torch.zeros_like(cosine)\n",
    "        one_hot.scatter_(1, label.view(-1, 1), 1.0)\n",
    "\n",
    "        # Apply arc margin only to the correct class logits\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "adac6577",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "class ResNetArcModel(nn.Module):\n",
    "    def __init__(self, num_classes, backbone=\"resnet50\", embedding_size=512):\n",
    "        \"\"\"\n",
    "        \n",
    "        Wraps a ResNet backbone and replaces final layer with embedding + ArcFace.\n",
    "        \n",
    "        Args:\n",
    "            num_classes: Number of output classes.\n",
    "            backbone: choose resnet version 'resnet18', 'resnet50'\n",
    "            embedding_size: Output dimension of the embedding before classification.\n",
    "        \"\"\"\n",
    "\n",
    "        super(ResNetArcModel, self).__init__()\n",
    "\n",
    "        # Load a PRETRAINED ResNet and remove the original classifier\n",
    "        resnet = getattr(models, backbone)(pretrained=True)\n",
    "        in_features = resnet.fc.in_features\n",
    "        resnet.fc = nn.Identity() # Remove final classification layer\n",
    "\n",
    "        self.backbone = resnet\n",
    "\n",
    "        # Project backbone output to lower-dim embedding\n",
    "        self.embedding = nn.Linear(in_features, embedding_size)\n",
    "\n",
    "        # ArcFace classification head\n",
    "        self.arcface = ArcFace(embedding_size, num_classes)\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the full model.\n",
    "\n",
    "        Args:\n",
    "            x: Input image tensor [B, C, H, W]\n",
    "            labels: Target labels [B], required for training\n",
    "        Returns:\n",
    "            If labels are provided: ArcFace logits (used in training).\n",
    "            If not: Raw embeddings (used in inference).\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.backbone(x)       # extract features from ResNet\n",
    "\n",
    "        x = self.embedding(x)      # project to embedding space\n",
    "\n",
    "        if labels is not None:\n",
    "            logits = self.arcface(x, labels)  # compute logits with arc margin\n",
    "            return logits\n",
    "        \n",
    "        return x  # inference mode: return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0b4177",
   "metadata": {},
   "source": [
    "EXAMPLE CODE TO USE IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6810cd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases encontradas: ['Abir Ahmed', 'Adriana Sanchez', 'Adriana Solanilla', 'Alejandro Tulipano', 'Amy Olivares', 'Blas de Leon', 'Carlos Beitia', 'Carlos Hernandez', 'Cesar Rodriguez', 'Javier Bustamante', 'Jeremy Sanchez', 'Jonathan Peralta', 'Kevin Rodriguez', 'Mahir Arcia', 'Michael Jordan']\n",
      "Número de clases: 15\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "# Transformaciones (debes usar las mismas que en inferencia)\n",
    "# 2. Define transform (match training time!)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "# Dataset\n",
    "train_dataset = ImageFolder(root=\"../data/preprocessed/train\", transform=transform)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "# Verifica clases\n",
    "print(\"Clases encontradas:\", train_dataset.classes)\n",
    "print(\"Número de clases:\", len(train_dataset.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23d5b3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 112, 112])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch[0].shape)  # imágenes\n",
    "    print(batch[1].shape)  # etiquetas\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4215b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/rodfer/Linux/entorno/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/media/rodfer/Linux/entorno/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# model = (num_classes=100)  cargar modelo base para entrenar\n",
    "model = ResNetArcModel(num_classes=15, embedding_size=512)\n",
    "model = model.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for images, labels in train_loader:\n",
    "    images = images.cuda()\n",
    "    labels = labels.cuda()\n",
    "\n",
    "    # Forward pass\n",
    "    logits = model(images, labels)\n",
    "    loss = criterion(logits, labels)\n",
    "\n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fa944d",
   "metadata": {},
   "source": [
    "TO SAVE THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4719117c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Loss: 0.0233: 100%|██████████| 75/75 [00:05<00:00, 13.13it/s]\n",
      "Epoch 2/10 Loss: 0.0404: 100%|██████████| 75/75 [00:05<00:00, 13.12it/s]\n",
      "Epoch 3/10 Loss: 0.0299: 100%|██████████| 75/75 [00:05<00:00, 13.04it/s]\n",
      "Epoch 4/10 Loss: 0.4453: 100%|██████████| 75/75 [00:05<00:00, 13.01it/s]\n",
      "Epoch 5/10 Loss: 0.2251: 100%|██████████| 75/75 [00:05<00:00, 13.01it/s]\n",
      "Epoch 6/10 Loss: 0.4108: 100%|██████████| 75/75 [00:05<00:00, 12.99it/s]\n",
      "Epoch 7/10 Loss: 0.1620: 100%|██████████| 75/75 [00:05<00:00, 12.99it/s]\n",
      "Epoch 8/10 Loss: 0.0152: 100%|██████████| 75/75 [00:05<00:00, 12.98it/s]\n",
      "Epoch 9/10 Loss: 0.0010: 100%|██████████| 75/75 [00:05<00:00, 12.96it/s]\n",
      "Epoch 10/10 Loss: 0.0433: 100%|██████████| 75/75 [00:05<00:00, 12.98it/s]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "epoch_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(train_loader)\n",
    "\n",
    "    for images, labels in pbar:\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        logits = model(images, labels)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        pbar.set_description(f\"Epoch {epoch+1}/{num_epochs} Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "470f8f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar checkpoint cada época\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'epoch': epoch + 1,\n",
    "}, f'../models/arcface_model_epoch_{epoch+1}.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed332de",
   "metadata": {},
   "source": [
    "TO LOAD THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23e8ee64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/rodfer/Linux/entorno/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/media/rodfer/Linux/entorno/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ResNetArcModel:\n\tMissing key(s) in state_dict: \"backbone.conv1.weight\", \"backbone.bn1.weight\", \"backbone.bn1.bias\", \"backbone.bn1.running_mean\", \"backbone.bn1.running_var\", \"backbone.layer1.0.conv1.weight\", \"backbone.layer1.0.bn1.weight\", \"backbone.layer1.0.bn1.bias\", \"backbone.layer1.0.bn1.running_mean\", \"backbone.layer1.0.bn1.running_var\", \"backbone.layer1.0.conv2.weight\", \"backbone.layer1.0.bn2.weight\", \"backbone.layer1.0.bn2.bias\", \"backbone.layer1.0.bn2.running_mean\", \"backbone.layer1.0.bn2.running_var\", \"backbone.layer1.0.conv3.weight\", \"backbone.layer1.0.bn3.weight\", \"backbone.layer1.0.bn3.bias\", \"backbone.layer1.0.bn3.running_mean\", \"backbone.layer1.0.bn3.running_var\", \"backbone.layer1.0.downsample.0.weight\", \"backbone.layer1.0.downsample.1.weight\", \"backbone.layer1.0.downsample.1.bias\", \"backbone.layer1.0.downsample.1.running_mean\", \"backbone.layer1.0.downsample.1.running_var\", \"backbone.layer1.1.conv1.weight\", \"backbone.layer1.1.bn1.weight\", \"backbone.layer1.1.bn1.bias\", \"backbone.layer1.1.bn1.running_mean\", \"backbone.layer1.1.bn1.running_var\", \"backbone.layer1.1.conv2.weight\", \"backbone.layer1.1.bn2.weight\", \"backbone.layer1.1.bn2.bias\", \"backbone.layer1.1.bn2.running_mean\", \"backbone.layer1.1.bn2.running_var\", \"backbone.layer1.1.conv3.weight\", \"backbone.layer1.1.bn3.weight\", \"backbone.layer1.1.bn3.bias\", \"backbone.layer1.1.bn3.running_mean\", \"backbone.layer1.1.bn3.running_var\", \"backbone.layer1.2.conv1.weight\", \"backbone.layer1.2.bn1.weight\", \"backbone.layer1.2.bn1.bias\", \"backbone.layer1.2.bn1.running_mean\", \"backbone.layer1.2.bn1.running_var\", \"backbone.layer1.2.conv2.weight\", \"backbone.layer1.2.bn2.weight\", \"backbone.layer1.2.bn2.bias\", \"backbone.layer1.2.bn2.running_mean\", \"backbone.layer1.2.bn2.running_var\", \"backbone.layer1.2.conv3.weight\", \"backbone.layer1.2.bn3.weight\", \"backbone.layer1.2.bn3.bias\", \"backbone.layer1.2.bn3.running_mean\", \"backbone.layer1.2.bn3.running_var\", \"backbone.layer2.0.conv1.weight\", \"backbone.layer2.0.bn1.weight\", \"backbone.layer2.0.bn1.bias\", \"backbone.layer2.0.bn1.running_mean\", \"backbone.layer2.0.bn1.running_var\", \"backbone.layer2.0.conv2.weight\", \"backbone.layer2.0.bn2.weight\", \"backbone.layer2.0.bn2.bias\", \"backbone.layer2.0.bn2.running_mean\", \"backbone.layer2.0.bn2.running_var\", \"backbone.layer2.0.conv3.weight\", \"backbone.layer2.0.bn3.weight\", \"backbone.layer2.0.bn3.bias\", \"backbone.layer2.0.bn3.running_mean\", \"backbone.layer2.0.bn3.running_var\", \"backbone.layer2.0.downsample.0.weight\", \"backbone.layer2.0.downsample.1.weight\", \"backbone.layer2.0.downsample.1.bias\", \"backbone.layer2.0.downsample.1.running_mean\", \"backbone.layer2.0.downsample.1.running_var\", \"backbone.layer2.1.conv1.weight\", \"backbone.layer2.1.bn1.weight\", \"backbone.layer2.1.bn1.bias\", \"backbone.layer2.1.bn1.running_mean\", \"backbone.layer2.1.bn1.running_var\", \"backbone.layer2.1.conv2.weight\", \"backbone.layer2.1.bn2.weight\", \"backbone.layer2.1.bn2.bias\", \"backbone.layer2.1.bn2.running_mean\", \"backbone.layer2.1.bn2.running_var\", \"backbone.layer2.1.conv3.weight\", \"backbone.layer2.1.bn3.weight\", \"backbone.layer2.1.bn3.bias\", \"backbone.layer2.1.bn3.running_mean\", \"backbone.layer2.1.bn3.running_var\", \"backbone.layer2.2.conv1.weight\", \"backbone.layer2.2.bn1.weight\", \"backbone.layer2.2.bn1.bias\", \"backbone.layer2.2.bn1.running_mean\", \"backbone.layer2.2.bn1.running_var\", \"backbone.layer2.2.conv2.weight\", \"backbone.layer2.2.bn2.weight\", \"backbone.layer2.2.bn2.bias\", \"backbone.layer2.2.bn2.running_mean\", \"backbone.layer2.2.bn2.running_var\", \"backbone.layer2.2.conv3.weight\", \"backbone.layer2.2.bn3.weight\", \"backbone.layer2.2.bn3.bias\", \"backbone.layer2.2.bn3.running_mean\", \"backbone.layer2.2.bn3.running_var\", \"backbone.layer2.3.conv1.weight\", \"backbone.layer2.3.bn1.weight\", \"backbone.layer2.3.bn1.bias\", \"backbone.layer2.3.bn1.running_mean\", \"backbone.layer2.3.bn1.running_var\", \"backbone.layer2.3.conv2.weight\", \"backbone.layer2.3.bn2.weight\", \"backbone.layer2.3.bn2.bias\", \"backbone.layer2.3.bn2.running_mean\", \"backbone.layer2.3.bn2.running_var\", \"backbone.layer2.3.conv3.weight\", \"backbone.layer2.3.bn3.weight\", \"backbone.layer2.3.bn3.bias\", \"backbone.layer2.3.bn3.running_mean\", \"backbone.layer2.3.bn3.running_var\", \"backbone.layer3.0.conv1.weight\", \"backbone.layer3.0.bn1.weight\", \"backbone.layer3.0.bn1.bias\", \"backbone.layer3.0.bn1.running_mean\", \"backbone.layer3.0.bn1.running_var\", \"backbone.layer3.0.conv2.weight\", \"backbone.layer3.0.bn2.weight\", \"backbone.layer3.0.bn2.bias\", \"backbone.layer3.0.bn2.running_mean\", \"backbone.layer3.0.bn2.running_var\", \"backbone.layer3.0.conv3.weight\", \"backbone.layer3.0.bn3.weight\", \"backbone.layer3.0.bn3.bias\", \"backbone.layer3.0.bn3.running_mean\", \"backbone.layer3.0.bn3.running_var\", \"backbone.layer3.0.downsample.0.weight\", \"backbone.layer3.0.downsample.1.weight\", \"backbone.layer3.0.downsample.1.bias\", \"backbone.layer3.0.downsample.1.running_mean\", \"backbone.layer3.0.downsample.1.running_var\", \"backbone.layer3.1.conv1.weight\", \"backbone.layer3.1.bn1.weight\", \"backbone.layer3.1.bn1.bias\", \"backbone.layer3.1.bn1.running_mean\", \"backbone.layer3.1.bn1.running_var\", \"backbone.layer3.1.conv2.weight\", \"backbone.layer3.1.bn2.weight\", \"backbone.layer3.1.bn2.bias\", \"backbone.layer3.1.bn2.running_mean\", \"backbone.layer3.1.bn2.running_var\", \"backbone.layer3.1.conv3.weight\", \"backbone.layer3.1.bn3.weight\", \"backbone.layer3.1.bn3.bias\", \"backbone.layer3.1.bn3.running_mean\", \"backbone.layer3.1.bn3.running_var\", \"backbone.layer3.2.conv1.weight\", \"backbone.layer3.2.bn1.weight\", \"backbone.layer3.2.bn1.bias\", \"backbone.layer3.2.bn1.running_mean\", \"backbone.layer3.2.bn1.running_var\", \"backbone.layer3.2.conv2.weight\", \"backbone.layer3.2.bn2.weight\", \"backbone.layer3.2.bn2.bias\", \"backbone.layer3.2.bn2.running_mean\", \"backbone.layer3.2.bn2.running_var\", \"backbone.layer3.2.conv3.weight\", \"backbone.layer3.2.bn3.weight\", \"backbone.layer3.2.bn3.bias\", \"backbone.layer3.2.bn3.running_mean\", \"backbone.layer3.2.bn3.running_var\", \"backbone.layer3.3.conv1.weight\", \"backbone.layer3.3.bn1.weight\", \"backbone.layer3.3.bn1.bias\", \"backbone.layer3.3.bn1.running_mean\", \"backbone.layer3.3.bn1.running_var\", \"backbone.layer3.3.conv2.weight\", \"backbone.layer3.3.bn2.weight\", \"backbone.layer3.3.bn2.bias\", \"backbone.layer3.3.bn2.running_mean\", \"backbone.layer3.3.bn2.running_var\", \"backbone.layer3.3.conv3.weight\", \"backbone.layer3.3.bn3.weight\", \"backbone.layer3.3.bn3.bias\", \"backbone.layer3.3.bn3.running_mean\", \"backbone.layer3.3.bn3.running_var\", \"backbone.layer3.4.conv1.weight\", \"backbone.layer3.4.bn1.weight\", \"backbone.layer3.4.bn1.bias\", \"backbone.layer3.4.bn1.running_mean\", \"backbone.layer3.4.bn1.running_var\", \"backbone.layer3.4.conv2.weight\", \"backbone.layer3.4.bn2.weight\", \"backbone.layer3.4.bn2.bias\", \"backbone.layer3.4.bn2.running_mean\", \"backbone.layer3.4.bn2.running_var\", \"backbone.layer3.4.conv3.weight\", \"backbone.layer3.4.bn3.weight\", \"backbone.layer3.4.bn3.bias\", \"backbone.layer3.4.bn3.running_mean\", \"backbone.layer3.4.bn3.running_var\", \"backbone.layer3.5.conv1.weight\", \"backbone.layer3.5.bn1.weight\", \"backbone.layer3.5.bn1.bias\", \"backbone.layer3.5.bn1.running_mean\", \"backbone.layer3.5.bn1.running_var\", \"backbone.layer3.5.conv2.weight\", \"backbone.layer3.5.bn2.weight\", \"backbone.layer3.5.bn2.bias\", \"backbone.layer3.5.bn2.running_mean\", \"backbone.layer3.5.bn2.running_var\", \"backbone.layer3.5.conv3.weight\", \"backbone.layer3.5.bn3.weight\", \"backbone.layer3.5.bn3.bias\", \"backbone.layer3.5.bn3.running_mean\", \"backbone.layer3.5.bn3.running_var\", \"backbone.layer4.0.conv1.weight\", \"backbone.layer4.0.bn1.weight\", \"backbone.layer4.0.bn1.bias\", \"backbone.layer4.0.bn1.running_mean\", \"backbone.layer4.0.bn1.running_var\", \"backbone.layer4.0.conv2.weight\", \"backbone.layer4.0.bn2.weight\", \"backbone.layer4.0.bn2.bias\", \"backbone.layer4.0.bn2.running_mean\", \"backbone.layer4.0.bn2.running_var\", \"backbone.layer4.0.conv3.weight\", \"backbone.layer4.0.bn3.weight\", \"backbone.layer4.0.bn3.bias\", \"backbone.layer4.0.bn3.running_mean\", \"backbone.layer4.0.bn3.running_var\", \"backbone.layer4.0.downsample.0.weight\", \"backbone.layer4.0.downsample.1.weight\", \"backbone.layer4.0.downsample.1.bias\", \"backbone.layer4.0.downsample.1.running_mean\", \"backbone.layer4.0.downsample.1.running_var\", \"backbone.layer4.1.conv1.weight\", \"backbone.layer4.1.bn1.weight\", \"backbone.layer4.1.bn1.bias\", \"backbone.layer4.1.bn1.running_mean\", \"backbone.layer4.1.bn1.running_var\", \"backbone.layer4.1.conv2.weight\", \"backbone.layer4.1.bn2.weight\", \"backbone.layer4.1.bn2.bias\", \"backbone.layer4.1.bn2.running_mean\", \"backbone.layer4.1.bn2.running_var\", \"backbone.layer4.1.conv3.weight\", \"backbone.layer4.1.bn3.weight\", \"backbone.layer4.1.bn3.bias\", \"backbone.layer4.1.bn3.running_mean\", \"backbone.layer4.1.bn3.running_var\", \"backbone.layer4.2.conv1.weight\", \"backbone.layer4.2.bn1.weight\", \"backbone.layer4.2.bn1.bias\", \"backbone.layer4.2.bn1.running_mean\", \"backbone.layer4.2.bn1.running_var\", \"backbone.layer4.2.conv2.weight\", \"backbone.layer4.2.bn2.weight\", \"backbone.layer4.2.bn2.bias\", \"backbone.layer4.2.bn2.running_mean\", \"backbone.layer4.2.bn2.running_var\", \"backbone.layer4.2.conv3.weight\", \"backbone.layer4.2.bn3.weight\", \"backbone.layer4.2.bn3.bias\", \"backbone.layer4.2.bn3.running_mean\", \"backbone.layer4.2.bn3.running_var\", \"embedding.weight\", \"embedding.bias\", \"arcface.weight\". \n\tUnexpected key(s) in state_dict: \"model_state_dict\", \"optimizer_state_dict\", \"epoch\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m model = ResNetArcModel(num_classes=\u001b[32m15\u001b[39m, embedding_size=\u001b[32m512\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model.load_state_dict(torch.load(\u001b[33m'\u001b[39m\u001b[33m../models/arcface_model_epoch_10.pth\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m      3\u001b[39m model = model.cuda()\n\u001b[32m      4\u001b[39m model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/rodfer/Linux/entorno/lib/python3.12/site-packages/torch/nn/modules/module.py:2593\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2585\u001b[39m         error_msgs.insert(\n\u001b[32m   2586\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2587\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2588\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2589\u001b[39m             ),\n\u001b[32m   2590\u001b[39m         )\n\u001b[32m   2592\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2593\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2594\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2595\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2596\u001b[39m         )\n\u001b[32m   2597\u001b[39m     )\n\u001b[32m   2598\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for ResNetArcModel:\n\tMissing key(s) in state_dict: \"backbone.conv1.weight\", \"backbone.bn1.weight\", \"backbone.bn1.bias\", \"backbone.bn1.running_mean\", \"backbone.bn1.running_var\", \"backbone.layer1.0.conv1.weight\", \"backbone.layer1.0.bn1.weight\", \"backbone.layer1.0.bn1.bias\", \"backbone.layer1.0.bn1.running_mean\", \"backbone.layer1.0.bn1.running_var\", \"backbone.layer1.0.conv2.weight\", \"backbone.layer1.0.bn2.weight\", \"backbone.layer1.0.bn2.bias\", \"backbone.layer1.0.bn2.running_mean\", \"backbone.layer1.0.bn2.running_var\", \"backbone.layer1.0.conv3.weight\", \"backbone.layer1.0.bn3.weight\", \"backbone.layer1.0.bn3.bias\", \"backbone.layer1.0.bn3.running_mean\", \"backbone.layer1.0.bn3.running_var\", \"backbone.layer1.0.downsample.0.weight\", \"backbone.layer1.0.downsample.1.weight\", \"backbone.layer1.0.downsample.1.bias\", \"backbone.layer1.0.downsample.1.running_mean\", \"backbone.layer1.0.downsample.1.running_var\", \"backbone.layer1.1.conv1.weight\", \"backbone.layer1.1.bn1.weight\", \"backbone.layer1.1.bn1.bias\", \"backbone.layer1.1.bn1.running_mean\", \"backbone.layer1.1.bn1.running_var\", \"backbone.layer1.1.conv2.weight\", \"backbone.layer1.1.bn2.weight\", \"backbone.layer1.1.bn2.bias\", \"backbone.layer1.1.bn2.running_mean\", \"backbone.layer1.1.bn2.running_var\", \"backbone.layer1.1.conv3.weight\", \"backbone.layer1.1.bn3.weight\", \"backbone.layer1.1.bn3.bias\", \"backbone.layer1.1.bn3.running_mean\", \"backbone.layer1.1.bn3.running_var\", \"backbone.layer1.2.conv1.weight\", \"backbone.layer1.2.bn1.weight\", \"backbone.layer1.2.bn1.bias\", \"backbone.layer1.2.bn1.running_mean\", \"backbone.layer1.2.bn1.running_var\", \"backbone.layer1.2.conv2.weight\", \"backbone.layer1.2.bn2.weight\", \"backbone.layer1.2.bn2.bias\", \"backbone.layer1.2.bn2.running_mean\", \"backbone.layer1.2.bn2.running_var\", \"backbone.layer1.2.conv3.weight\", \"backbone.layer1.2.bn3.weight\", \"backbone.layer1.2.bn3.bias\", \"backbone.layer1.2.bn3.running_mean\", \"backbone.layer1.2.bn3.running_var\", \"backbone.layer2.0.conv1.weight\", \"backbone.layer2.0.bn1.weight\", \"backbone.layer2.0.bn1.bias\", \"backbone.layer2.0.bn1.running_mean\", \"backbone.layer2.0.bn1.running_var\", \"backbone.layer2.0.conv2.weight\", \"backbone.layer2.0.bn2.weight\", \"backbone.layer2.0.bn2.bias\", \"backbone.layer2.0.bn2.running_mean\", \"backbone.layer2.0.bn2.running_var\", \"backbone.layer2.0.conv3.weight\", \"backbone.layer2.0.bn3.weight\", \"backbone.layer2.0.bn3.bias\", \"backbone.layer2.0.bn3.running_mean\", \"backbone.layer2.0.bn3.running_var\", \"backbone.layer2.0.downsample.0.weight\", \"backbone.layer2.0.downsample.1.weight\", \"backbone.layer2.0.downsample.1.bias\", \"backbone.layer2.0.downsample.1.running_mean\", \"backbone.layer2.0.downsample.1.running_var\", \"backbone.layer2.1.conv1.weight\", \"backbone.layer2.1.bn1.weight\", \"backbone.layer2.1.bn1.bias\", \"backbone.layer2.1.bn1.running_mean\", \"backbone.layer2.1.bn1.running_var\", \"backbone.layer2.1.conv2.weight\", \"backbone.layer2.1.bn2.weight\", \"backbone.layer2.1.bn2.bias\", \"backbone.layer2.1.bn2.running_mean\", \"backbone.layer2.1.bn2.running_var\", \"backbone.layer2.1.conv3.weight\", \"backbone.layer2.1.bn3.weight\", \"backbone.layer2.1.bn3.bias\", \"backbone.layer2.1.bn3.running_mean\", \"backbone.layer2.1.bn3.running_var\", \"backbone.layer2.2.conv1.weight\", \"backbone.layer2.2.bn1.weight\", \"backbone.layer2.2.bn1.bias\", \"backbone.layer2.2.bn1.running_mean\", \"backbone.layer2.2.bn1.running_var\", \"backbone.layer2.2.conv2.weight\", \"backbone.layer2.2.bn2.weight\", \"backbone.layer2.2.bn2.bias\", \"backbone.layer2.2.bn2.running_mean\", \"backbone.layer2.2.bn2.running_var\", \"backbone.layer2.2.conv3.weight\", \"backbone.layer2.2.bn3.weight\", \"backbone.layer2.2.bn3.bias\", \"backbone.layer2.2.bn3.running_mean\", \"backbone.layer2.2.bn3.running_var\", \"backbone.layer2.3.conv1.weight\", \"backbone.layer2.3.bn1.weight\", \"backbone.layer2.3.bn1.bias\", \"backbone.layer2.3.bn1.running_mean\", \"backbone.layer2.3.bn1.running_var\", \"backbone.layer2.3.conv2.weight\", \"backbone.layer2.3.bn2.weight\", \"backbone.layer2.3.bn2.bias\", \"backbone.layer2.3.bn2.running_mean\", \"backbone.layer2.3.bn2.running_var\", \"backbone.layer2.3.conv3.weight\", \"backbone.layer2.3.bn3.weight\", \"backbone.layer2.3.bn3.bias\", \"backbone.layer2.3.bn3.running_mean\", \"backbone.layer2.3.bn3.running_var\", \"backbone.layer3.0.conv1.weight\", \"backbone.layer3.0.bn1.weight\", \"backbone.layer3.0.bn1.bias\", \"backbone.layer3.0.bn1.running_mean\", \"backbone.layer3.0.bn1.running_var\", \"backbone.layer3.0.conv2.weight\", \"backbone.layer3.0.bn2.weight\", \"backbone.layer3.0.bn2.bias\", \"backbone.layer3.0.bn2.running_mean\", \"backbone.layer3.0.bn2.running_var\", \"backbone.layer3.0.conv3.weight\", \"backbone.layer3.0.bn3.weight\", \"backbone.layer3.0.bn3.bias\", \"backbone.layer3.0.bn3.running_mean\", \"backbone.layer3.0.bn3.running_var\", \"backbone.layer3.0.downsample.0.weight\", \"backbone.layer3.0.downsample.1.weight\", \"backbone.layer3.0.downsample.1.bias\", \"backbone.layer3.0.downsample.1.running_mean\", \"backbone.layer3.0.downsample.1.running_var\", \"backbone.layer3.1.conv1.weight\", \"backbone.layer3.1.bn1.weight\", \"backbone.layer3.1.bn1.bias\", \"backbone.layer3.1.bn1.running_mean\", \"backbone.layer3.1.bn1.running_var\", \"backbone.layer3.1.conv2.weight\", \"backbone.layer3.1.bn2.weight\", \"backbone.layer3.1.bn2.bias\", \"backbone.layer3.1.bn2.running_mean\", \"backbone.layer3.1.bn2.running_var\", \"backbone.layer3.1.conv3.weight\", \"backbone.layer3.1.bn3.weight\", \"backbone.layer3.1.bn3.bias\", \"backbone.layer3.1.bn3.running_mean\", \"backbone.layer3.1.bn3.running_var\", \"backbone.layer3.2.conv1.weight\", \"backbone.layer3.2.bn1.weight\", \"backbone.layer3.2.bn1.bias\", \"backbone.layer3.2.bn1.running_mean\", \"backbone.layer3.2.bn1.running_var\", \"backbone.layer3.2.conv2.weight\", \"backbone.layer3.2.bn2.weight\", \"backbone.layer3.2.bn2.bias\", \"backbone.layer3.2.bn2.running_mean\", \"backbone.layer3.2.bn2.running_var\", \"backbone.layer3.2.conv3.weight\", \"backbone.layer3.2.bn3.weight\", \"backbone.layer3.2.bn3.bias\", \"backbone.layer3.2.bn3.running_mean\", \"backbone.layer3.2.bn3.running_var\", \"backbone.layer3.3.conv1.weight\", \"backbone.layer3.3.bn1.weight\", \"backbone.layer3.3.bn1.bias\", \"backbone.layer3.3.bn1.running_mean\", \"backbone.layer3.3.bn1.running_var\", \"backbone.layer3.3.conv2.weight\", \"backbone.layer3.3.bn2.weight\", \"backbone.layer3.3.bn2.bias\", \"backbone.layer3.3.bn2.running_mean\", \"backbone.layer3.3.bn2.running_var\", \"backbone.layer3.3.conv3.weight\", \"backbone.layer3.3.bn3.weight\", \"backbone.layer3.3.bn3.bias\", \"backbone.layer3.3.bn3.running_mean\", \"backbone.layer3.3.bn3.running_var\", \"backbone.layer3.4.conv1.weight\", \"backbone.layer3.4.bn1.weight\", \"backbone.layer3.4.bn1.bias\", \"backbone.layer3.4.bn1.running_mean\", \"backbone.layer3.4.bn1.running_var\", \"backbone.layer3.4.conv2.weight\", \"backbone.layer3.4.bn2.weight\", \"backbone.layer3.4.bn2.bias\", \"backbone.layer3.4.bn2.running_mean\", \"backbone.layer3.4.bn2.running_var\", \"backbone.layer3.4.conv3.weight\", \"backbone.layer3.4.bn3.weight\", \"backbone.layer3.4.bn3.bias\", \"backbone.layer3.4.bn3.running_mean\", \"backbone.layer3.4.bn3.running_var\", \"backbone.layer3.5.conv1.weight\", \"backbone.layer3.5.bn1.weight\", \"backbone.layer3.5.bn1.bias\", \"backbone.layer3.5.bn1.running_mean\", \"backbone.layer3.5.bn1.running_var\", \"backbone.layer3.5.conv2.weight\", \"backbone.layer3.5.bn2.weight\", \"backbone.layer3.5.bn2.bias\", \"backbone.layer3.5.bn2.running_mean\", \"backbone.layer3.5.bn2.running_var\", \"backbone.layer3.5.conv3.weight\", \"backbone.layer3.5.bn3.weight\", \"backbone.layer3.5.bn3.bias\", \"backbone.layer3.5.bn3.running_mean\", \"backbone.layer3.5.bn3.running_var\", \"backbone.layer4.0.conv1.weight\", \"backbone.layer4.0.bn1.weight\", \"backbone.layer4.0.bn1.bias\", \"backbone.layer4.0.bn1.running_mean\", \"backbone.layer4.0.bn1.running_var\", \"backbone.layer4.0.conv2.weight\", \"backbone.layer4.0.bn2.weight\", \"backbone.layer4.0.bn2.bias\", \"backbone.layer4.0.bn2.running_mean\", \"backbone.layer4.0.bn2.running_var\", \"backbone.layer4.0.conv3.weight\", \"backbone.layer4.0.bn3.weight\", \"backbone.layer4.0.bn3.bias\", \"backbone.layer4.0.bn3.running_mean\", \"backbone.layer4.0.bn3.running_var\", \"backbone.layer4.0.downsample.0.weight\", \"backbone.layer4.0.downsample.1.weight\", \"backbone.layer4.0.downsample.1.bias\", \"backbone.layer4.0.downsample.1.running_mean\", \"backbone.layer4.0.downsample.1.running_var\", \"backbone.layer4.1.conv1.weight\", \"backbone.layer4.1.bn1.weight\", \"backbone.layer4.1.bn1.bias\", \"backbone.layer4.1.bn1.running_mean\", \"backbone.layer4.1.bn1.running_var\", \"backbone.layer4.1.conv2.weight\", \"backbone.layer4.1.bn2.weight\", \"backbone.layer4.1.bn2.bias\", \"backbone.layer4.1.bn2.running_mean\", \"backbone.layer4.1.bn2.running_var\", \"backbone.layer4.1.conv3.weight\", \"backbone.layer4.1.bn3.weight\", \"backbone.layer4.1.bn3.bias\", \"backbone.layer4.1.bn3.running_mean\", \"backbone.layer4.1.bn3.running_var\", \"backbone.layer4.2.conv1.weight\", \"backbone.layer4.2.bn1.weight\", \"backbone.layer4.2.bn1.bias\", \"backbone.layer4.2.bn1.running_mean\", \"backbone.layer4.2.bn1.running_var\", \"backbone.layer4.2.conv2.weight\", \"backbone.layer4.2.bn2.weight\", \"backbone.layer4.2.bn2.bias\", \"backbone.layer4.2.bn2.running_mean\", \"backbone.layer4.2.bn2.running_var\", \"backbone.layer4.2.conv3.weight\", \"backbone.layer4.2.bn3.weight\", \"backbone.layer4.2.bn3.bias\", \"backbone.layer4.2.bn3.running_mean\", \"backbone.layer4.2.bn3.running_var\", \"embedding.weight\", \"embedding.bias\", \"arcface.weight\". \n\tUnexpected key(s) in state_dict: \"model_state_dict\", \"optimizer_state_dict\", \"epoch\". "
     ]
    }
   ],
   "source": [
    "model = ResNetArcModel(num_classes=15, embedding_size=512)\n",
    "model.load_state_dict(torch.load('../models/arcface_model_epoch_10.pth'))\n",
    "model = model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677b1473",
   "metadata": {},
   "source": [
    "TO SAVE EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6823757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.nn.functional import normalize\n",
    "from face_recognition import face_locations\n",
    "from model import ResNetWithArcFace  # Your ArcFace model class\n",
    "\n",
    "# 1. Load trained model\n",
    "model = ResNetArcModel(num_classes=15, embedding_size=512)\n",
    "model.load_state_dict(torch.load('../models/arcface_model_epoch_10.pth'))\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# 2. Define transform (match training time!)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "# 3. Function to extract and compute embedding\n",
    "def extract_embedding_from_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Detect face\n",
    "    face_coords = face_locations(rgb)\n",
    "    if not face_coords:\n",
    "        raise ValueError(\"No face found in image.\")\n",
    "    \n",
    "    top, right, bottom, left = face_coords[0]\n",
    "    face = image[top:bottom, left:right]\n",
    "\n",
    "    # Transform and get embedding\n",
    "    face_tensor = transform(face).unsqueeze(0).cuda()\n",
    "    with torch.no_grad():\n",
    "        embedding = model(face_tensor)\n",
    "        embedding = normalize(embedding, dim=1)  # cosine normalization\n",
    "    return embedding.squeeze(0).cpu().numpy()  # 512-dim vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67a190b",
   "metadata": {},
   "source": [
    "EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b91799",
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_embedding = extract_embedding_from_image('alice.jpg')\n",
    "np.save('alice_embedding.npy', alice_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7370e007",
   "metadata": {},
   "source": [
    "PRACTICAL USE IN REAL TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41117501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.nn.functional import normalize\n",
    "from face_recognition import face_locations  # uses dlib internally\n",
    "from model import ResNetWithArcFace  # your model class\n",
    "\n",
    "# Load your trained model\n",
    "model = ResNetWithArcFace(num_classes=100, embedding_size=512)\n",
    "model.load_state_dict(torch.load('arcface_model_weights.pth'))\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# Transformation for input images (resize, tensor, normalize)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "# Load reference embeddings (enrolled people)\n",
    "reference_db = {\n",
    "    \"Alice\": np.load(\"alice_embedding.npy\"),\n",
    "    \"Bob\": np.load(\"bob_embedding.npy\")\n",
    "    # Add more enrolled users\n",
    "}\n",
    "\n",
    "# Function to compute embedding from face crop\n",
    "def get_embedding(face_img):\n",
    "    face_tensor = transform(face_img).unsqueeze(0).cuda()\n",
    "    with torch.no_grad():\n",
    "        emb = model(face_tensor)  # returns [1, 512]\n",
    "        emb = normalize(emb, dim=1)\n",
    "    return emb.squeeze(0).cpu().numpy()  # [512]\n",
    "\n",
    "# Compare to reference embeddings\n",
    "def recognize_face(embedding, threshold=0.5):\n",
    "    best_match = None\n",
    "    best_score = -1\n",
    "    for name, ref_emb in reference_db.items():\n",
    "        score = np.dot(embedding, ref_emb)  # cosine similarity\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_match = name\n",
    "    if best_score >= threshold:\n",
    "        return best_match, best_score\n",
    "    else:\n",
    "        return \"Unknown\", best_score\n",
    "\n",
    "# OpenCV capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "print(\"[INFO] Starting camera...\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    faces = face_locations(rgb_frame)  # returns (top, right, bottom, left)\n",
    "\n",
    "    for (top, right, bottom, left) in faces:\n",
    "        face_img = frame[top:bottom, left:right]  # crop\n",
    "        if face_img.size == 0:\n",
    "            continue\n",
    "\n",
    "        emb = get_embedding(face_img)\n",
    "        name, score = recognize_face(emb)\n",
    "\n",
    "        # Draw bounding box and name\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f'{name} ({score:.2f})', (left, top - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"ArcFace Recognition\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entorno",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
