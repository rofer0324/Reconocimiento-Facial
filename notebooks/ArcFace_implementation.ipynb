{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06c4ba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a7c144",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcFace(nn.Module):\n",
    "    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False):\n",
    "        \"\"\"\n",
    "        \n",
    "        Implementacion de ArcFace\n",
    "        \n",
    "        Args:\n",
    "            in_features: Tamaño del embedding vector (input)\n",
    "            out_features: Números de clases\n",
    "            s: Factor de escalado\n",
    "            m: margen añadido entre clases en el espacio angular\n",
    "            easy_margin: Usar solamente si la version base se vuelve inestable\n",
    "        \"\"\"\n",
    "\n",
    "        super(ArcFace, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight) # inicializa la clase para los weights\n",
    "\n",
    "        # Calcula cos(m) y sen(m) para temas de eficencia\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = math.cos(math.pi - m) # umbral para el margen de decision\n",
    "        self.mm = math.sin(math.pi - m) * m # margen de penalización\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        \"\"\"\n",
    "        \n",
    "        Forward prop para ArcFace\n",
    "        \n",
    "        Args:\n",
    "            input: Inputs las dimensiones del embedding tensor [batch_size, in_features]\n",
    "            label: Labels con la dimension del [batch_size]\n",
    "        Returns:\n",
    "            Output: Logits (resultado tras pasar por una funcion de activacion) with shape [batch_size, out_features] to pass it to the CrossEntropyLoss (Ya me cansé de escribir en español)\n",
    "        \"\"\" \n",
    "\n",
    "        # Normalize both inputs features and weight matrix\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight)) # Cosine similarity between features and weights\n",
    "        sine = torch.sqrt(1.0 - torch.clamp(cosine**2, 0, 1)) # sin(θ) from cos(θ)\n",
    "\n",
    "        # Compute cos(θ + m) using trigonometric identity\n",
    "        phi = cosine + self.cos_m - sine * self.sin_m\n",
    "\n",
    "        # Decide whether to apply margin based on thresholding (remember just use it, if the model becomes unstable)\n",
    "        if self.easy_margin:\n",
    "            # Use cosine if it is positive, else keep original\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            # Use original phi only if above threshold, else apply modified margin\n",
    "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        \n",
    "        # One Hot to enconde labels\n",
    "        one_hot = torch.zeros_like(cosine)\n",
    "        one_hot.scatter_(1, label.view(-1, 1), 1.0)\n",
    "\n",
    "        # Apply arc margin only to the correct class logits\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adac6577",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "class ResNetArcModel(nn.Module):\n",
    "    def __init__(self, num_classes, backbone=\"resnet50\", embedding_size=512):\n",
    "        \"\"\"\n",
    "        \n",
    "        Wraps a ResNet backbone and replaces final layer with embedding + ArcFace.\n",
    "        \n",
    "        Args:\n",
    "            num_classes: Number of output classes.\n",
    "            backbone: choose resnet version 'resnet18', 'resnet50'\n",
    "            embedding_size: Output dimension of the embedding before classification.\n",
    "        \"\"\"\n",
    "\n",
    "        super(ResNetArcModel, self).__init__()\n",
    "\n",
    "        # Load a PRETRAINED ResNet and remove the original classifier\n",
    "        resnet = getattr(models, backbone)(pretrainable=True)\n",
    "        in_features = resnet.fc.in_features\n",
    "        resnet.fc = nn.Identity() # Remove final classification layer\n",
    "\n",
    "        self.backbone = resnet\n",
    "\n",
    "        # Project backbone output to lower-dim embedding\n",
    "        self.embedding = nn.Linear(in_features, embedding_size)\n",
    "\n",
    "        # ArcFace classification head\n",
    "        self.arcface = ArcFace(embedding_size, num_classes)\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the full model.\n",
    "\n",
    "        Args:\n",
    "            x: Input image tensor [B, C, H, W]\n",
    "            labels: Target labels [B], required for training\n",
    "        Returns:\n",
    "            If labels are provided: ArcFace logits (used in training).\n",
    "            If not: Raw embeddings (used in inference).\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.backbone(x)       # extract features from ResNet\n",
    "\n",
    "        x = self.embedding(x)      # project to embedding space\n",
    "\n",
    "        if labels is not None:\n",
    "            logits = self.arcface(x, labels)  # compute logits with arc margin\n",
    "            return logits\n",
    "        \n",
    "        return x  # inference mode: return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0b4177",
   "metadata": {},
   "source": [
    "EXAMPLE CODE TO USE IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4215b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = (num_classes=100)  cargar modelo base para entrenar\n",
    "model = model.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for images, labels in train_loader:\n",
    "    images = images.cuda()\n",
    "    labels = labels.cuda()\n",
    "\n",
    "    # Forward pass\n",
    "    logits = model(images, labels)\n",
    "    loss = criterion(logits, labels)\n",
    "\n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fa944d",
   "metadata": {},
   "source": [
    "TO SAVE THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4719117c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'epoch': current_epoch,\n",
    "}, 'arcface_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed332de",
   "metadata": {},
   "source": [
    "TO LOAD THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e8ee64",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNetWithArcFace(num_classes=100, embedding_size=512)\n",
    "model.load_state_dict(torch.load('arcface_model_weights.pth'))\n",
    "model = model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677b1473",
   "metadata": {},
   "source": [
    "TO SAVE EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6823757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.nn.functional import normalize\n",
    "from face_recognition import face_locations\n",
    "from model import ResNetWithArcFace  # Your ArcFace model class\n",
    "\n",
    "# 1. Load trained model\n",
    "model = ResNetWithArcFace(num_classes=100, embedding_size=512)\n",
    "model.load_state_dict(torch.load('arcface_model_weights.pth'))\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# 2. Define transform (match training time!)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "# 3. Function to extract and compute embedding\n",
    "def extract_embedding_from_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Detect face\n",
    "    face_coords = face_locations(rgb)\n",
    "    if not face_coords:\n",
    "        raise ValueError(\"No face found in image.\")\n",
    "    \n",
    "    top, right, bottom, left = face_coords[0]\n",
    "    face = image[top:bottom, left:right]\n",
    "\n",
    "    # Transform and get embedding\n",
    "    face_tensor = transform(face).unsqueeze(0).cuda()\n",
    "    with torch.no_grad():\n",
    "        embedding = model(face_tensor)\n",
    "        embedding = normalize(embedding, dim=1)  # cosine normalization\n",
    "    return embedding.squeeze(0).cpu().numpy()  # 512-dim vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67a190b",
   "metadata": {},
   "source": [
    "EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b91799",
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_embedding = extract_embedding_from_image('alice.jpg')\n",
    "np.save('alice_embedding.npy', alice_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7370e007",
   "metadata": {},
   "source": [
    "PRACTICAL USE IN REAL TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41117501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.nn.functional import normalize\n",
    "from face_recognition import face_locations  # uses dlib internally\n",
    "from model import ResNetWithArcFace  # your model class\n",
    "\n",
    "# Load your trained model\n",
    "model = ResNetWithArcFace(num_classes=100, embedding_size=512)\n",
    "model.load_state_dict(torch.load('arcface_model_weights.pth'))\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# Transformation for input images (resize, tensor, normalize)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "# Load reference embeddings (enrolled people)\n",
    "reference_db = {\n",
    "    \"Alice\": np.load(\"alice_embedding.npy\"),\n",
    "    \"Bob\": np.load(\"bob_embedding.npy\")\n",
    "    # Add more enrolled users\n",
    "}\n",
    "\n",
    "# Function to compute embedding from face crop\n",
    "def get_embedding(face_img):\n",
    "    face_tensor = transform(face_img).unsqueeze(0).cuda()\n",
    "    with torch.no_grad():\n",
    "        emb = model(face_tensor)  # returns [1, 512]\n",
    "        emb = normalize(emb, dim=1)\n",
    "    return emb.squeeze(0).cpu().numpy()  # [512]\n",
    "\n",
    "# Compare to reference embeddings\n",
    "def recognize_face(embedding, threshold=0.5):\n",
    "    best_match = None\n",
    "    best_score = -1\n",
    "    for name, ref_emb in reference_db.items():\n",
    "        score = np.dot(embedding, ref_emb)  # cosine similarity\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_match = name\n",
    "    if best_score >= threshold:\n",
    "        return best_match, best_score\n",
    "    else:\n",
    "        return \"Unknown\", best_score\n",
    "\n",
    "# OpenCV capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "print(\"[INFO] Starting camera...\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    faces = face_locations(rgb_frame)  # returns (top, right, bottom, left)\n",
    "\n",
    "    for (top, right, bottom, left) in faces:\n",
    "        face_img = frame[top:bottom, left:right]  # crop\n",
    "        if face_img.size == 0:\n",
    "            continue\n",
    "\n",
    "        emb = get_embedding(face_img)\n",
    "        name, score = recognize_face(emb)\n",
    "\n",
    "        # Draw bounding box and name\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f'{name} ({score:.2f})', (left, top - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"ArcFace Recognition\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asistencia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
