{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0596830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21060104",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcFace(nn.Module):\n",
    "    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False):\n",
    "        super(ArcFace, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        sine = torch.sqrt(1.0 - torch.clamp(cosine**2, 0, 1))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "\n",
    "        one_hot = torch.zeros_like(cosine)\n",
    "        one_hot.scatter_(1, label.view(-1,1), 1.0)\n",
    "\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9978284a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetArcModel(nn.Module):\n",
    "    def __init__(self, num_classes, backbone=\"resnet50\", embedding_size=512):\n",
    "        super(ResNetArcModel, self).__init__()\n",
    "        resnet = getattr(models, backbone)(pretrained=True)\n",
    "        in_features = resnet.fc.in_features\n",
    "        resnet.fc = nn.Identity()\n",
    "\n",
    "        self.backbone = resnet\n",
    "        self.embedding = nn.Linear(in_features, embedding_size)\n",
    "        self.arcface = ArcFace(embedding_size, num_classes)\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        x = self.backbone(x)\n",
    "        x = self.embedding(x)\n",
    "        if labels is not None:\n",
    "            logits = self.arcface(x, labels)\n",
    "            return logits\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3726b5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases: ['Abir Ahmed', 'Adriana Sanchez', 'Adriana Solanilla', 'Alejandro Tulipano', 'Amy Olivares', 'Blas de Leon', 'Carlos Beitia', 'Carlos Hernandez', 'Cesar Rodriguez', 'Javier Bustamante', 'Jeremy Sanchez', 'Jonathan Peralta', 'Kevin Rodriguez', 'Mahir Arcia', 'Michael Jordan']\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((112,112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    root=\"../data/preprocessed/train\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "num_classes = len(train_dataset.classes)\n",
    "print(\"Clases:\", train_dataset.classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc14160",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNetArcModel(num_classes=num_classes, embedding_size=512).cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228bf5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Loss: 0.0607: 100%|██████████| 75/75 [00:05<00:00, 13.45it/s]\n",
      "Epoch 2/10 Loss: 0.0610: 100%|██████████| 75/75 [00:05<00:00, 13.35it/s]\n",
      "Epoch 3/10 Loss: 0.0437: 100%|██████████| 75/75 [00:05<00:00, 13.36it/s]\n",
      "Epoch 4/10 Loss: 0.0960: 100%|██████████| 75/75 [00:05<00:00, 13.31it/s]\n",
      "Epoch 5/10 Loss: 0.0772: 100%|██████████| 75/75 [00:05<00:00, 13.29it/s]\n",
      "Epoch 6/10 Loss: 0.0493: 100%|██████████| 75/75 [00:05<00:00, 13.19it/s]\n",
      "Epoch 7/10 Loss: 0.0691: 100%|██████████| 75/75 [00:05<00:00, 13.25it/s]\n",
      "Epoch 8/10 Loss: 0.0644: 100%|██████████| 75/75 [00:05<00:00, 13.22it/s]\n",
      "Epoch 9/10 Loss: 0.0773: 100%|██████████| 75/75 [00:05<00:00, 13.16it/s]\n",
      "Epoch 10/10 Loss: 0.0365: 100%|██████████| 75/75 [00:05<00:00, 13.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modelo completo guardado.\n",
      "✅ Backbone+embedding guardado.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(train_loader)\n",
    "    \n",
    "    for images, labels in pbar:\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        logits = model(images, labels)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        pbar.set_description(f\"Epoch {epoch+1}/{num_epochs} Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Save the whole model\n",
    "torch.save(model.state_dict(), \"../models/arcface_model_10.pth\")\n",
    "print(\"✅ Modelo completo guardado.\")\n",
    "\n",
    "# Save only backbone+embedding (without ArcFace head)\n",
    "torch.save(\n",
    "    {\n",
    "        \"backbone\": model.backbone.state_dict(),\n",
    "        \"embedding\": model.embedding.state_dict()\n",
    "    },\n",
    "    \"../models/arcface_backbone.pth\"\n",
    ")\n",
    "print(\"✅ Backbone+embedding guardado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7325b1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/rodfer/Linux/entorno/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando Abir Ahmed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 241.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando Adriana Sanchez...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 258.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando Adriana Solanilla...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 272.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando Alejandro Tulipano...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 267.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando Amy Olivares...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 268.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando Blas de Leon...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 233.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando Carlos Beitia...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 258.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando Carlos Hernandez...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:00<00:00, 258.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando Cesar Rodriguez...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 152.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando Javier Bustamante...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 136.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando Jeremy Sanchez...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 144.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando Jonathan Peralta...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 141.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando Kevin Rodriguez...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 143.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando Mahir Arcia...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 147.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando Michael Jordan...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 144.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Galería guardada en ../models/gallery_embeddings.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATASET_DIR = \"../data/preprocessed/test\"\n",
    "\n",
    "# Cargar backbone\n",
    "backbone = models.resnet50(pretrained=False)\n",
    "in_features = backbone.fc.in_features\n",
    "backbone.fc = nn.Identity()\n",
    "checkpoint = torch.load(\"../models/arcface_backbone.pth\")\n",
    "backbone.load_state_dict(checkpoint[\"backbone\"])\n",
    "\n",
    "embedding_layer = torch.nn.Linear(in_features, 512)\n",
    "embedding_layer.load_state_dict(checkpoint[\"embedding\"])\n",
    "\n",
    "backbone = backbone.to(DEVICE).eval()\n",
    "embedding_layer = embedding_layer.to(DEVICE).eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((112,112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "class_embeddings = {}\n",
    "\n",
    "for class_name in os.listdir(DATASET_DIR):\n",
    "    class_dir = os.path.join(DATASET_DIR, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "\n",
    "    embeddings = []\n",
    "    image_files = [f for f in os.listdir(class_dir) if f.lower().endswith((\".jpg\", \".png\"))]\n",
    "\n",
    "    print(f\"Procesando {class_name}...\")\n",
    "\n",
    "    for img_file in tqdm(image_files):\n",
    "        img_path = os.path.join(class_dir, img_file)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        x = transform(img).unsqueeze(0).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            features = backbone(x)\n",
    "            emb = embedding_layer(features)\n",
    "            emb = F.normalize(emb, dim=1)\n",
    "        embeddings.append(emb.squeeze(0).cpu())\n",
    "\n",
    "    mean_emb = torch.stack(embeddings).mean(0)\n",
    "    mean_emb = F.normalize(mean_emb, dim=0)\n",
    "    class_embeddings[class_name] = mean_emb.numpy()\n",
    "\n",
    "np.save(\"../models/gallery_embeddings.npy\", class_embeddings)\n",
    "print(\"✅ Galería guardada en ../models/gallery_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5e0e3c",
   "metadata": {},
   "source": [
    "## INFERENCIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2c5e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1751394116.636541   17807 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torch.nn.functional import normalize\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Cargar backbone\n",
    "backbone = models.resnet50(pretrained=False)\n",
    "in_features = backbone.fc.in_features\n",
    "backbone.fc = nn.Identity()\n",
    "checkpoint = torch.load(\"../models/arcface_backbone.pth\")\n",
    "backbone.load_state_dict(checkpoint[\"backbone\"])\n",
    "\n",
    "embedding_layer = torch.nn.Linear(in_features, 512)\n",
    "embedding_layer.load_state_dict(checkpoint[\"embedding\"])\n",
    "\n",
    "backbone = backbone.to(DEVICE).eval()\n",
    "embedding_layer = embedding_layer.to(DEVICE).eval()\n",
    "\n",
    "# Transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((112,112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "# Galería de embeddings\n",
    "reference_db = np.load(\"../models/gallery_embeddings.npy\", allow_pickle=True).item()\n",
    "\n",
    "# Mediapipe detection\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "face_detector = mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.9)\n",
    "\n",
    "def get_embedding(face_img):\n",
    "    face_tensor = transform(face_img).unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        features = backbone(face_tensor)\n",
    "        emb = embedding_layer(features)\n",
    "        emb = normalize(emb, dim=1)\n",
    "    return emb.squeeze(0).cpu().numpy()\n",
    "\n",
    "def recognize_face(embedding, threshold=0.91):\n",
    "    best_match = None\n",
    "    best_score = -1\n",
    "    for name, ref_emb in reference_db.items():\n",
    "        score = np.dot(embedding, ref_emb)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_match = name\n",
    "    if best_score >= threshold:\n",
    "        return best_match\n",
    "    return \"Desconocido\"\n",
    "\n",
    "# Cambiar aquí\n",
    "# cap = cv2.VideoCapture(0)  # Webcam\n",
    "cap = cv2.VideoCapture(\"../data/crudo/Abir1.mp4\")  # Video\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_detector.process(rgb)\n",
    "\n",
    "    if results.detections:\n",
    "        for det in results.detections:\n",
    "            bbox = det.location_data.relative_bounding_box\n",
    "            ih, iw, _ = frame.shape\n",
    "            x1 = max(int(bbox.xmin * iw), 0)\n",
    "            y1 = max(int(bbox.ymin * ih), 0)\n",
    "            w = int(bbox.width * iw)\n",
    "            h = int(bbox.height * ih)\n",
    "            x2 = min(x1 + w, iw)\n",
    "            y2 = min(y1 + h, ih)\n",
    "\n",
    "            face_img = frame[y1:y2, x1:x2]\n",
    "            if face_img.size == 0:\n",
    "                continue\n",
    "\n",
    "            # Reconocimiento\n",
    "            emb = get_embedding(face_img)\n",
    "            name = recognize_face(emb)\n",
    "\n",
    "            # Bounding box\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0,255,0), 2)\n",
    "            cv2.putText(frame, name, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,255,255), 2)\n",
    "\n",
    "            # Dibujar keypoints\n",
    "            for kp in det.location_data.relative_keypoints:\n",
    "                kp_x = int(kp.x * iw)\n",
    "                kp_y = int(kp.y * ih)\n",
    "                cv2.circle(frame, (kp_x, kp_y), 2, (0, 250, 0), -1)\n",
    "\n",
    "    cv2.imshow(\"Recognition\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entorno",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
